### k-近邻算法
采用测量不同特征值之间的距离方法进行分类
> * **优点** 精度高、对异常值不敏感、无数据输入假定
> * **缺点** 计算复杂度高、空间复杂度高。
> * **使用数据类型** 数值型和标称型  

流程
> 1. 收集数据 可以使用任何方法
> 2. 准备数据 距离计算所需要的数值， 最好使结构化的数据格式
> 3. 分析数据 可以使用任何方法
> 4. 训练算法 此步骤不适用k近邻算法
> 5. 测试算法 计算错误率
> 6. 使用算法 首先需要输入样本数据和结构化的输出结构，然后运行k近邻算法判定输入数据分别属于那个分类，最后应用对计算出的分类执行后续的处理

### 决策树

 > * __优点__ 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
 > * __缺点__ 可能会产生过度匹配的问题
 > * __使用数据类型__ 数值型和标称型

 流程
 > 1. 收集数据 可以使用任何方法
 > 2. 准备数据 树构造算法只适用于标称数据，因此数值型数据必须离散化
 > 3. 分析数据 可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期
 > 4. 训练算法 构造树的数据结构
 > 5. 测试算法 使用经验树计算错误率
 > 6. 使用算法 此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义

划分数据集的大原则是`将无序的数据变得更加有序`  
**熵** 信息的期望值
> [熵 (信息论)](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))

###基于概率论的分类方法：朴素贝叶斯·
> * __优点__ 在数据较少的情况下仍然有效，可以处理多类别问题
> * __缺点__ 对于输入数据的准备方式较为敏感
> * __使用数据类型__ 标称型

流程
> 1. 收集数据 可以使用任何方法
> 2. 准备数据 需要数值型或者布尔型数据
> 3. 分析数据 有大量特征时，绘制特征作用不大，此时使用直方图效果更好
> 4. 训练算法 计算不同的独立特征条件概率
> 5. 测试算法 计算错误率
> 6. 使用算法 一个常见的朴素贝叶斯应用时文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要时文本

### Logistic回归
流程
> 1. 收集数据：采用任意方法收集数据。
> 2. 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式最佳。
> 3. 分析数据：采用任意方法对数据进行分析。
> 4. 训练算法：大部分时间将用于训练， 训练的目的是为了找到最佳的分类回归系数。
> 5. 测试算法：一旦训练步骤完成，分类将会很快。
> 6. 使用算法：首先，我们需要一些输入数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于那个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。
