### k-近邻算法
采用测量不同特征值之间的距离方法进行分类
> * **优点** 精度高、对异常值不敏感、无数据输入假定
> * **缺点** 计算复杂度高、空间复杂度高。
> * **使用数据类型** 数值型和标称型  

流程
> 1. 收集数据 可以使用任何方法
> 2. 准备数据 距离计算所需要的数值， 最好使结构化的数据格式
> 3. 分析数据 可以使用任何方法
> 4. 训练算法 此步骤不适用k近邻算法
> 5. 测试算法 计算错误率
> 6. 使用算法 首先需要输入样本数据和结构化的输出结构，然后运行k近邻算法判定输入数据分别属于那个分类，最后应用对计算出的分类执行后续的处理

###决策树
 > * __优点__ 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
 > * __缺点__ 可能会产生过度匹配的问题
 > * __使用数据类型__ 数值型和标称型

 流程
 > 1. 收集数据 可以使用任何方法
 > 2. 准备数据 树构造算法只适用于标称数据，因此数值型数据必须离散化
 > 3. 分析数据 可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期
 > 4. 训练算法 构造树的数据结构
 > 5. 测试算法 使用经验树计算错误率
 > 6. 使用算法 此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义

划分数据集的大原则是`将无序的数据变得更加有序`  
**熵** 信息的期望值
> [熵 (信息论)](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))

###基于概率论的分类方法：朴素贝叶斯
> * __优点__ 在数据较少的情况下仍然有效，可以处理多类别问题
> * __缺点__ 对于输入数据的准备方式较为敏感
> * __使用数据类型__ 标称型

流程
> 1. 收集数据 可以使用任何方法
> 2. 准备数据 需要数值型或者布尔型数据
> 3. 分析数据 有大量特征时，绘制特征作用不大，此时使用直方图效果更好
> 4. 训练算法 计算不同的独立特征条件概率
> 5. 测试算法 计算错误率
> 6. 使用算法 一个常见的朴素贝叶斯应用时文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要时文本
