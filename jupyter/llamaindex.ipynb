{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install llama-index\n",
    "pip install langchain\n",
    "pip install unstructured\n",
    "pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, T5Tokenizer, T5ForConditionalGeneration, GPT2TokenizerFast\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "print(prompt)\n",
    "\n",
    "model_path = \"/Users/warriorg/Downloads/bert-base-chinese\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "hf = HuggingFaceEmbeddings(model_name=model_path, model_kwargs={\"device\": \"cpu\"})\n",
    "\n",
    "query_result = hf.embed_query('你好')\n",
    "print(query_result)\n",
    "\n",
    "\n",
    "\n",
    "#max_length has typically been deprecated for max_new_tokens \n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, model_kwargs={\"temperature\":0}\n",
    "# )\n",
    "# hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=hf)\n",
    "# question = \"hi\"\n",
    "# result = llm_chain.run(question)\n",
    "# print(\"Answer: \" + result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "from llama_index import SimpleKeywordTableIndex\n",
    "import os\n",
    "\n",
    "MarkdownReader = download_loader(\"MarkdownReader\")\n",
    "\n",
    "loader = MarkdownReader()\n",
    "data = loader.load_data(file=os.path.join(\"/Users/warriorg/Workspace/Notes/doc/customs\", \"关务.md\"))\n",
    "index = SimpleKeywordTableIndex.from_documents(data)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"B/L\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 加载文件夹中的所有txt类型的文件\n",
    "loader = DirectoryLoader('/Users/warriorg/Downloads/', glob='**/*.txt')\n",
    "# 将数据转成 document 对象，每个文件会作为一个 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化加载器\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# 切割加载的 document\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 初始化 openai 的 embeddings 对象\n",
    "model_path = \"/Users/warriorg/Downloads/bert-base-chinese\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_path, model_kwargs={\"device\": \"cpu\"})\n",
    "# 将 document 通过 openai 的 embeddings 对象计算 embedding 向量信息并临时存入 Chroma 向量数据库，用于后续匹配查询\n",
    "docsearch = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "# 创建问答对象\n",
    "qa = RetrievalQA.from_chain_type(llm=LLMChain(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True)\n",
    "# 进行问答\n",
    "result = qa({\"query\": \"科大讯飞今年第一季度收入是多少？\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 Hugging Face 模型直接拉到本地使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(local_llm('What is the capital of France? '))\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm)\n",
    "question = \"What is the capital of England?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"chavinlo/alpaca-native\",\n",
    "    load_in_8bit=False,\n",
    "    device_map='cpu',\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
